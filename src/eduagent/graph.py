from __future__ import annotations
from dotenv import load_dotenv
from typing import Annotated, List, Optional
from dataclasses import dataclass

from langchain_core.runnables import RunnableConfig
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph

load_dotenv()  # Ä‘áº£m báº£o Ä‘á»c OPENAI_API_KEY sá»›m


# âœ¨ ThÃªm OpenAI
from langchain_openai import ChatOpenAI

# === Reducer Ä‘Ãºng chuáº©n ===
def last_5_msgs(a: List[BaseMessage], b: List[BaseMessage]) -> List[BaseMessage]:
    return (a + b)[-5:]

@dataclass
class State:
    messages: Annotated[List[BaseMessage], last_5_msgs]
    next_agent: Optional[str] = None


# === Agent Nodes ===

def planner_agent(state: State, config: RunnableConfig) -> dict:
    """Sá»­ dá»¥ng LLM Ä‘á»ƒ suy luáº­n bÆ°á»›c tiáº¿p theo vÃ  chá»n agent phÃ¹ há»£p."""
    print("ğŸ” [Planner Agent] Suy nghÄ© káº¿ hoáº¡ch...")

    planning_prompt = state.messages + [
        HumanMessage(
            content=(
                "Dá»±a vÃ o há»™i thoáº¡i trÃªn, hÃ£y lÃªn káº¿ hoáº¡ch bÆ°á»›c tiáº¿p theo cho há»‡ thá»‘ng. "
                "Chá»n má»™t trong cÃ¡c agent sau Ä‘á»ƒ xá»­ lÃ½: visual, teacher, rag. "
                "Tráº£ lá»i duy nháº¥t báº±ng JSON vá»›i hai khÃ³a 'plan' vÃ  'next_agent'."
            )
        )
    ]

    response = llm.invoke(planning_prompt)

    try:
        import json

        parsed = json.loads(response.content)
        plan_text = parsed.get("plan", "")
        next_agent = parsed.get("next_agent", "teacher")
    except Exception:
        plan_text = response.content
        next_agent = "teacher"

    msg = AIMessage(content=plan_text)
    return {"messages": state.messages + [msg], "next_agent": next_agent}


# âœ¨ Gá»i LLM thá»±c táº¿ tá»« OpenAI
llm = ChatOpenAI(model_name="gpt-4.1", temperature=0.7)

def teacher_agent(state: State, config: RunnableConfig) -> dict:
    """Gá»i GPT-4 Ä‘á»ƒ tráº£ lá»i kiáº¿n thá»©c cho giÃ¡o viÃªn / há»c sinh."""
    print("ğŸ“˜ [Teacher Agent] Gá»i OpenAIâ€¦")

    # DÃ¹ng toÃ n bá»™ lá»‹ch sá»­ há»™i thoáº¡i lÃ m ngá»¯ cáº£nh
    response = llm.invoke(state.messages)

    # Náº¿u chá»‰ muá»‘n dÃ¹ng tin nháº¯n cuá»‘i cÃ¹ng:
    #   response = llm.invoke(state.messages[-1].content)

    return {"messages": state.messages + [response]}


def visual_agent(state: State, config: RunnableConfig) -> dict:
    print("ğŸ–¼ï¸ [Visual Agent] Táº¡o ná»™i dung trá»±c quan...")
    msg = AIMessage(content="ÄÃ¢y lÃ  ná»™i dung trá»±c quan cho yÃªu cáº§u cá»§a báº¡n.")
    return {"messages": state.messages + [msg]}


def rag_agent(state: State, config: RunnableConfig) -> dict:
    print("ğŸ“š [RAG Agent] Truy xuáº¥t thÃ´ng tin giÃ¡o dá»¥c...")
    msg = AIMessage(content="TÃ´i Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u phÃ¹ há»£p vá»›i chá»§ Ä‘á» báº¡n há»i.")
    return {"messages": state.messages + [msg]}


def finish(state: State, config: RunnableConfig) -> dict:
    print("âœ… [End] Káº¿t thÃºc phiÃªn tráº£ lá»i.")
    return {"messages": state.messages}


# === Build Graph ===

graph = StateGraph(State)

graph.add_node("planner", planner_agent)
graph.add_node("teacher", teacher_agent)
graph.add_node("visual", visual_agent)
graph.add_node("rag", rag_agent)
graph.add_node("end", finish)

graph.set_entry_point("planner")
graph.add_conditional_edges(
    "planner",
    lambda state: state.next_agent,
    {
        "teacher": "teacher",
        "visual": "visual",
        "rag": "rag",
    },
)
graph.add_edge("teacher", "end")
graph.add_edge("visual", "end")
graph.add_edge("rag", "end")

graph = graph.compile()
