from __future__ import annotations
import os
from dotenv import load_dotenv
from typing import TypedDict, Annotated, List, Union
from dataclasses import dataclass, field

from langchain_core.runnables import RunnableConfig
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END

load_dotenv()  # Ä‘áº£m báº£o Ä‘á»c OPENAI_API_KEY sá»›m


# âœ¨ ThÃªm OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

# Thá»­ dÃ¹ng vector store tá»« langchain_community; náº¿u khÃ´ng cÃ³ thÃ¬ sáº½ fallback
try:  # pragma: no cover - phá»¥ thuá»™c vÃ o mÃ´i trÆ°á»ng
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import FakeEmbeddings
except Exception:  # pragma: no cover - fallback náº¿u thiáº¿u gÃ³i
    Chroma = None  # type: ignore
    FakeEmbeddings = None  # type: ignore

# === Reducer Ä‘Ãºng chuáº©n ===
def last_5_msgs(a: List[BaseMessage], b: List[BaseMessage]) -> List[BaseMessage]:
    return (a + b)[-5:]

@dataclass
class State:
    messages: Annotated[List[BaseMessage], last_5_msgs]
    memory: List[str] = field(default_factory=list)


# === Agent Nodes ===

def planner_agent(state: State, config: RunnableConfig) -> dict:
    print("ğŸ” [Planner Agent] Suy nghÄ© káº¿ hoáº¡ch...")
    msg = AIMessage(content="TÃ´i Ä‘Ã£ hiá»ƒu yÃªu cáº§u. Äá»ƒ tÃ´i lÃªn káº¿ hoáº¡ch cho báº¡n.")
    return {
        "messages": state.messages + [msg],
        "memory": state.memory + ["Planner Ä‘Ã£ Ä‘Æ°a ra káº¿ hoáº¡ch."],
    }


# âœ¨ Gá»i LLM thá»±c táº¿ tá»« OpenAI
llm = ChatOpenAI(model_name="gpt-4.1", temperature=0.7)

def teacher_agent(state: State, config: RunnableConfig) -> dict:
    """Gá»i GPT-4 Ä‘á»ƒ tráº£ lá»i kiáº¿n thá»©c cho giÃ¡o viÃªn / há»c sinh."""
    print("ğŸ“˜ [Teacher Agent] Gá»i OpenAIâ€¦")

    # DÃ¹ng toÃ n bá»™ lá»‹ch sá»­ há»™i thoáº¡i lÃ m ngá»¯ cáº£nh
    try:
        response = llm.invoke(state.messages)
    except Exception:  # pragma: no cover - fallback khi khÃ´ng gá»i Ä‘Æ°á»£c API
        response = AIMessage(content="(LLM khÃ´ng kháº£ dá»¥ng, sá»­ dá»¥ng tráº£ lá»i máº·c Ä‘á»‹nh.)")

    # Náº¿u chá»‰ muá»‘n dÃ¹ng tin nháº¯n cuá»‘i cÃ¹ng:
    #   response = llm.invoke(state.messages[-1].content)

    return {
        "messages": state.messages + [response],
        "memory": state.memory + [f"Teacher tráº£ lá»i: {response.content}"],
    }


def parent_coach_agent(state: State, config: RunnableConfig) -> dict:
    print("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ [Parent Coach Agent] Gá»£i Ã½ cho phá»¥ huynh...")
    msg = AIMessage(content="Gá»£i Ã½: hÃ£y cÃ¹ng con luyá»‡n táº­p 15 phÃºt má»—i ngÃ y vÃ  há»i con xem con hiá»ƒu bÃ i chÆ°a.")
    return {
        "messages": state.messages + [msg],
        "memory": state.memory + ["Parent Coach Ä‘Ã£ gá»£i Ã½ luyá»‡n táº­p."],
    }


if Chroma and FakeEmbeddings:  # pragma: no cover
    # DÃ¹ng FakeEmbeddings Ä‘á»ƒ trÃ¡nh phá»¥ thuá»™c vÃ o API ngoÃ i khi cháº¡y thá»­
    embeddings = FakeEmbeddings(size=512)

    # Má»™t sá»‘ tÃ i liá»‡u vÃ­ dá»¥ trong cÆ¡ sá»Ÿ tri thá»©c
    _docs = [
        Document(page_content="ToÃ¡n há»c lÃ  ná»n táº£ng cho nhiá»u ngÃ nh khoa há»c khÃ¡c."),
        Document(page_content="Khoa há»c lá»‹ch sá»­ giÃºp há»c sinh hiá»ƒu vá» nguá»“n gá»‘c dÃ¢n tá»™c."),
    ]

    # Khá»Ÿi táº¡o retriever sá»­ dá»¥ng Chroma
    _retriever = Chroma.from_documents(_docs, embeddings).as_retriever()
else:  # Náº¿u thiáº¿u thÆ° viá»‡n, dÃ¹ng retrieval Ä‘Æ¡n giáº£n báº±ng tÃ¬m kiáº¿m chuá»—i
    _docs = [
        Document(page_content="ToÃ¡n há»c lÃ  ná»n táº£ng cho nhiá»u ngÃ nh khoa há»c khÃ¡c."),
        Document(page_content="Khoa há»c lá»‹ch sá»­ giÃºp há»c sinh hiá»ƒu vá» nguá»“n gá»‘c dÃ¢n tá»™c."),
    ]

    def _retriever(query: str):  # type: ignore
        results = [
            d for d in _docs if any(word.lower() in d.page_content.lower() for word in query.split())
        ]
        return results or _docs


def rag_agent(state: State, config: RunnableConfig) -> dict:
    print("ğŸ“š [RAG Agent] Truy xuáº¥t thÃ´ng tin giÃ¡o dá»¥c...")
    query = state.messages[-1].content
    if callable(getattr(_retriever, "invoke", None)):
        docs = _retriever.invoke(query)
    else:  # _retriever lÃ  hÃ m fallback
        docs = _retriever(query)  # type: ignore
    top_content = docs[0].page_content if docs else "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u phÃ¹ há»£p."
    msg = AIMessage(
        content=f"TÃ´i Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u phÃ¹ há»£p: {top_content}",
    )
    return {
        "messages": state.messages + [msg],
        "memory": state.memory + [f"RAG káº¿t quáº£: {top_content}"],
    }


def finish(state: State, config: RunnableConfig) -> dict:
    print("âœ… [End] Káº¿t thÃºc phiÃªn tráº£ lá»i.")
    return {"messages": state.messages, "memory": state.memory}


# === Build Graph ===

graph = StateGraph(State)

graph.add_node("planner", planner_agent)
graph.add_node("teacher", teacher_agent)
graph.add_node("parent", parent_coach_agent)
graph.add_node("rag", rag_agent)
graph.add_node("end", finish)

graph.set_entry_point("planner")
graph.add_edge("planner", "teacher")
graph.add_edge("teacher", "rag")
graph.add_edge("rag", "parent")
graph.add_edge("parent", "end")

graph = graph.compile()
